{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "In this notebook, we will pre-process the data to prepare it for the model. This will greatly reduce the amount of time needed to parse the collection for the IR systems.\n",
    "\n",
    "---\n",
    "\n",
    "## Import Required Libraries\n",
    "\n",
    "Import the necessary libraries, including pandas and matplotlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textprocessor import ReferenceRearrangedTextProcessor, ReferenceTextProcessor\n",
    "from textprocessor import CustomTextProcessorNoStopNoStem\n",
    "from textprocessor import CustomTextProcessorNoStem\n",
    "from textprocessor import CustomTextProcessorNoStop\n",
    "from textprocessor import CustomTextProcessor\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = CustomTextProcessorNoStopNoStem()\n",
    "\n",
    "# XML-Coll-withSem\n",
    "collection_name = \"XML-Coll-withSem\"\n",
    "processed_dir = (\n",
    "    \"../lib/processed_data/\"\n",
    "    + collection_name\n",
    "    + \"_\"\n",
    "    + processor.get_text_processor_name()\n",
    "    + \"/\"\n",
    ")\n",
    "\n",
    "os.makedirs(processed_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = \"../lib/data/practice_05/\" + collection_name + \".zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to retrieve the text between the xml tags and then we process the text using our `TextProcessor` class. The pre-processed text is then put back to recreate the xml file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_in_tags(element):\n",
    "    if element.text:\n",
    "        processed_text = \" \".join(processor.pre_processing(element.text))\n",
    "        element.text = processed_text\n",
    "\n",
    "    if element.tail:  # Process the tail (text after the closing tag)\n",
    "        processed_tail = \" \".join(processor.pre_processing(element.tail))\n",
    "        element.tail = processed_tail\n",
    "\n",
    "    for child in element:\n",
    "        process_text_in_tags(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 9804/9804 [03:06<00:00, 52.53it/s]\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "    xml_file_name = zip_ref.namelist()\n",
    "    for file in tqdm(xml_file_name, desc=\"Processing files\"):\n",
    "        with zip_ref.open(file) as xml_file:\n",
    "            xml_content = xml_file.read().decode(\"utf-8\")\n",
    "\n",
    "            xml_content = re.sub(r\"<\\/?b>\", \"\", xml_content)\n",
    "            xml_content = re.sub(r\"<\\/?it>\", \"\", xml_content)\n",
    "\n",
    "            root = ET.fromstring(re.sub(\"&[^;]+;\", \" \", xml_content))\n",
    "            process_text_in_tags(root)\n",
    "\n",
    "            # Convert the processed XML content to a string\n",
    "            processed_xml = ET.tostring(root, encoding=\"unicode\")\n",
    "            processed_xml_file_path = (\n",
    "                f\"{processed_dir}{file.split('/')[1].split('.')[0]}.xml\"\n",
    "            )\n",
    "\n",
    "            # Save the processed XML content to a new XML file\n",
    "            with open(processed_xml_file_path, \"w\", encoding=\"utf-8\") as processed_file:\n",
    "                processed_file.write(processed_xml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A small analysis\n",
    "\n",
    "We want to check what are the differences between the new pre-processing and the old one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_content(element):\n",
    "    # Start with an empty string\n",
    "    text_content = \"\"\n",
    "    # Loop through each child element\n",
    "    for child in element:\n",
    "        # If the child is a sub-element, recursively get its text content\n",
    "        if len(child) > 0:\n",
    "            text_content += get_text_content(child)\n",
    "        # Append the text of the current element\n",
    "        if child.text:\n",
    "            text_content += child.text\n",
    "        # Append any tail text after the element\n",
    "        if child.tail:\n",
    "            text_content += child.tail\n",
    "    return text_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../lib/data/practice_05/medium/54772.xml\", \"rb\") as file:\n",
    "    content = file.read().decode(\"utf-8\")\n",
    "    root = ET.fromstring(re.sub(\"&[^;]+;\", \" \", content))\n",
    "    content = \" \".join(root.itertext())\n",
    "    # content = get_text_content(root)\n",
    "    content = processor.pre_processing(content)\n",
    "    with open(\"./original/54772-2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\" \".join(content))\n",
    "\n",
    "\n",
    "with open(\"../lib/processed_data/medium_stop671_porter/54772.xml\", \"rb\") as file:\n",
    "    xml_content = file.read().decode(\"utf-8\")\n",
    "    root = ET.fromstring(xml_content)\n",
    "    xml_content = \" \".join(root.itertext())\n",
    "\n",
    "with open(\"../lib/data/practice_05/medium/54772.xml\", \"rb\") as file:\n",
    "    content = file.read().decode(\"utf-8\")\n",
    "\n",
    "    # remove all the xml tags\n",
    "    content = re.sub(\"<[^>]*>\", \"\", content)\n",
    "    content = re.sub(\"&[^;]+;\", \" \", content)\n",
    "    text = processor.pre_processing(content)\n",
    "\n",
    "    with open(\"./original/54772.txt\", \"w\") as f:\n",
    "        f.write(\" \".join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff(\"./original/54772.txt\", xml_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read xml files\n",
    "all_stats_df = pd.DataFrame()\n",
    "# Read xml files and compare with text files\n",
    "with zipfile.ZipFile(\"../lib/processed_data/medium_stop671_porter.zip\", \"r\") as zip_ref:\n",
    "    xml_file_name = zip_ref.namelist()\n",
    "    for file in tqdm(xml_file_name, desc=\"Processing files\"):\n",
    "        with zip_ref.open(file) as xml_file:\n",
    "            xml_content = xml_file.read().decode(\"utf-8\")\n",
    "            root = ET.fromstring(xml_content)\n",
    "            xml_content = \" \".join(root.itertext())\n",
    "\n",
    "            # Generate differences and store in DataFrame\n",
    "            df = diff(f\"./original/{file.split('/')[1].split('.')[0]}.txt\", xml_content)\n",
    "\n",
    "            # Extract 'docno' from the file name\n",
    "            docno = file.split(\"/\")[1].split(\".\")[0]\n",
    "\n",
    "            # Add 'docno' as a column\n",
    "            df[\"docno\"] = docno\n",
    "\n",
    "            # Append the DataFrame to the main DataFrame\n",
    "            all_stats_df = pd.concat([all_stats_df, df], ignore_index=True)\n",
    "\n",
    "# Reorder columns with 'docno' as the first column\n",
    "all_stats_df = all_stats_df[\n",
    "    [\"docno\"] + [col for col in all_stats_df.columns if col != \"docno\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stats_df = all_stats_df[all_stats_df[\"Operation\"] != \"\"]\n",
    "all_stats_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for changement in 153299 for file 1\n",
    "all_stats_df[\"File 1\"] = \"px\"\n",
    "all_stats_df[all_stats_df[\"docno\"] == \"153299\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create Counters for inserted, deleted, and replaced words\n",
    "inserted_words_counter = Counter()\n",
    "deleted_words_counter = Counter()\n",
    "replaced_words_counter = Counter()\n",
    "\n",
    "# Process rows in the DataFrame to count words based on operation type\n",
    "for index, row in all_stats_df.iterrows():\n",
    "    operation = row[\"Operation\"]\n",
    "    file1_words = row[\"File 1\"].split() if isinstance(row[\"File 1\"], str) else []\n",
    "    file2_words = row[\"File 2\"].split() if isinstance(row[\"File 2\"], str) else []\n",
    "\n",
    "    # Increment word counts based on operation type\n",
    "    if operation == \"Inserted\":\n",
    "        inserted_words_counter.update(file2_words)\n",
    "    elif operation == \"Deleted\":\n",
    "        deleted_words_counter.update(file1_words)\n",
    "    elif operation == \"Replaced\":\n",
    "        # Count words from both file1 and file2 as replaced words\n",
    "        replaced_words_counter.update(file1_words + file2_words)\n",
    "\n",
    "# Display the most common deleted, inserted, and replaced words\n",
    "print(\"Most common words deleted:\")\n",
    "print(deleted_words_counter.most_common(10))\n",
    "\n",
    "print(\"\\nMost common words inserted:\")\n",
    "print(inserted_words_counter.most_common(10))\n",
    "\n",
    "print(\"\\nMost common words replaced:\")\n",
    "print(replaced_words_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total words removed\n",
    "total_words_removed = sum(deleted_words_counter.values())\n",
    "print(f\"Total words removed: {total_words_removed}\")\n",
    "\n",
    "# total words inserted\n",
    "total_words_inserted = sum(inserted_words_counter.values())\n",
    "print(f\"Total words inserted: {total_words_inserted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of all the removed words\n",
    "removed_words_freq = deleted_words_counter.items()\n",
    "# sort the words by frequency\n",
    "removed_words_freq = sorted(removed_words_freq, key=lambda x: x[1], reverse=True)\n",
    "print(f\"Frequency of all removed words: {removed_words_freq}\")\n",
    "\n",
    "# sum of the frequency of the removed words\n",
    "removed_words_freq_sum = sum([freq for _, freq in removed_words_freq])\n",
    "print(f\"Sum of the frequency of the removed words: {removed_words_freq_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the removed words and their frequencies\n",
    "removed_words, removed_counts = zip(*removed_words_freq)\n",
    "\n",
    "# Plotting the frequency of removed words\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(removed_words[:20], removed_counts[:20], color=\"red\")\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Frequency of Removed Words (Top 20)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
