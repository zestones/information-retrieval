\documentclass[a4paper, 12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{moreverb}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{url}

\usepackage{geometry}
\usepackage{booktabs}

\geometry{a4paper, total={6in, 9in}}
\def\UrlBreaks{\do\/\do-}  % Permet des sauts de ligne dans les URL au niveau des "/" et des "-"

\usepackage{float}
\usepackage[a4paper,left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}

\pagestyle{headings}
\pagestyle{plain}

\usepackage{listings} 


\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\makeatletter


\makeatother

\makeatletter
\def\toclevel@subsubsubsection{4}
\def\toclevel@paragraph{5}
\def\toclevel@subparagraph{6}
\makeatother

\setlength{\parindent}{0cm}
\setlength{\parskip}{1ex plus 0.5ex minus 0.2ex}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}


\begin{document}

\begin{titlepage}
  \begin{sffamily}
  \begin{center}

   
    \textsc{\LARGE }\\[2cm]

    \textsc{\Large M2 DSC - Université Jean Monnet}\\[1.5cm]

    % Title
    \HRule \\[0.4cm]
    { \huge  \textsc{Information Retrieval} \\
    \textsc{\Large Rapport de projet} \\[0.4cm] }
	
    \HRule \\[2cm]
    \textsc {Idriss BENGUEZZOU \\Ghilas MEZIANE}
 \begin{figure}
     \centering
    \includegraphics[scale=0.2]{logoUJM.png}
     \label{fig:ujm_logo}
 \end{figure}

    \vfill

    % Bottom of the page
    {\large {} 02/02/2024}

  \end{center}
  \end{sffamily}
\end{titlepage}

\newpage

\tableofcontents

\newpage

\section{Introduction}
Ce projet a pour but de mettre en pratique les notions vues en cours et en TD. 
Il s'agit de réaliser un système de recherche d'information sur un corpus de documents, en utilisant les méthodes vues en cours. 
Le corpus de documents est constitué de 9804 documents, et les requêtes sont au nombre de sept.

L'idée est de répondre aux exigences spécifiques de compétitions comme l'INEX, qui imposent des standards stricts pour évaluer les performances des systèmes de recherche d'information.
Notre rapport détaillera notre démarche, les choix méthodologiques, les paramétrages utilisés, et les résultats obtenus, en mettant particulièrement 
l'accent sur notre démarche de recherche de la meilleure configuration possible pour notre système de recherche d'information.


\section{Méthodologie}

Nous avons choisi de travailler avec le langage de programmation Python, au vu de sa popularité et de sa richesse en bibliothèques pour le traitement de texte. 
En outre, aucune bibliothèque n'a été utilisée pour la recherche d'information, l'indexation, ou le calcul de similarité. Nous allons détailler notre démarche pour chaque étape du projet.

\subsection{Prétraitement des données}

Lors de nos expérimentations, nous avons constaté que l'étape de prétraitement des données est cruciale pour la qualité des résultats obtenus.
En effet, tout au long de nos expérimentations, nous avons constaté que les résultats obtenus étaient très sensibles à la qualité du prétraitement des données. 
Avec un prétraitement de qualité, nous avons pu améliorer notre score de MaGP de 0.198 à 0,2512. 

Tout d'abord, nous avons utilisé la bibliothèque \texttt{nltk} pour le prétraitement des données.
Nous avons utilisé la fonction \texttt{word\_tokenize} pour la tokenisation des documents, et la fonction \texttt{PorterStemmer} pour la racinisation des mots.
Nous avons également utilisé la liste de stopwords fournie en cours pour élimer les mots vides.

\subsubsection{Prétraitement des données initial}

Notre implémentation initiale des étapes de prétraitement des données a été la suivante:

\begin{enumerate}
  \item Tokenisation
  \item Mise en minuscule
  \item Racinisation des mots (stemming)
  \item Élimination des mots vides (suppression des stopwords)
  \item Élimination de la ponctuation
  \item Élimination des nombres
  \item Élimination des caractères spéciaux
  \item Élimination des mots chaines de caractères vides  
\end{enumerate}

Ces étapes de prétraitement des données était réaliser à chaque fois que nous devions indexer les documents.
Nous verrons plus tard que par soucis d'optimisation des temps, nous avons choisi de réaliser ces étapes de prétraitement des données une seule fois, et de sauvegarder les résultats dans une nouvelle collection.
Nous avons nommé cette première méthode de prétraitement des données : \texttt{Processing Reference},
car elle est réalisée à chaque fois que nous devons indexer les documents.
Voici les statistiques de notre collection après cette première méthode de prétraitement des données:

\begin{table}[!h]
  \begin{minipage}{0.35\linewidth}
      \centering
      \resizebox{\textwidth}{!}{%
          \begin{tabular}{|c|c|c|}
              \hline
              \textbf{Statistiques} & \textbf{Valeur} \\
              \hline
              Taille moyenne des documents &  654.008 \\
              Taille moyenne des mots &  8.391 \\
              Nombre de mots unique &  210,232 \\
              Nombre de mots total &  6.41189e+06 \\
              \hline
          \end{tabular}%
      }
      \caption{Statistiques de la collection}
  \end{minipage}%
  \hspace{0.05\linewidth} % Add a 5% width margin
  \begin{minipage}{0.60\linewidth}
      Ces valeurs sont obtenues après avoir réalisé les étapes de prétraitement des données décrites ci-dessus.
      Elles nous ont permis d'avoir une référérence pour la suite de nos expérimentations, et de comparer les résultats obtenus avec d'autres méthodes de prétraitement des données.
      Une run de notre système de recherche d'information avec ces valeurs nous a donné un score de MaGP de 0.198.
  \end{minipage}
\end{table}

\subsubsection{Optimisation du Prétraitement des Données : Expérimentations avec un Ordre Modifié des Étapes}

Suite à quelque expérimentations, nous avons constaté plusieurs choses qui nous ont poussé à modifier l'ordre des étapes de prétraitement des données.
Nous avons remarqué que l'étape de \texttt{tokenisation} ne fonctionnait pas correctement par moment, par exemple:
En tokenisant la phrase "World Ltd. World Ltd." nous obtenions ["World", "Ltd", ".", "World", "Ltd."] au lieu de ["world", "ltd", ".", "world", "ltd", "."]. 
En effet, en menant quelques recherches, nous avons remarqué que la tokenisation pouvais ne pas fonctionner correctement avec les abréviations, les mots vides, et les mots composés.
Nous avons donc décidé de modifier l'ordre des étapes de prétraitement des données, et de réaliser la tokenisation après l'élimination de la ponctuation, des nombres, et des caractères spéciaux.

De plus, en parcourant notre liste de mots vides (stopwords), nous avons remarqué que tout les mots présent dans 
cette liste était en minuscule. Donc il était nécessaire de mettre en minuscule les mots avant de chercher à
éliminer les mots vides. C'est à ce moment que nous avons remarqué que l'étape de racinisation des mots (stemming)
était réalisée avant l'étape de suppression des mots vides. Ce qui fait que les mots vides n'étaient pas éliminés
correctement. Nous avons donc décidé de réaliser la racinisation des mots après l'élimination des mots vides.

Notre nouvelle méthode de prétraitement des données est la suivante:
\begin{enumerate}
  \item Élimination de la ponctuation
  \item Élimination des nombres
  \item Élimination des caractères spéciaux
  \item Tokenisation
  \item Mise en minuscule
  \item Élimination des mots vides (suppression des stopwords)
  \item Racinisation des mots (stemming)
  \item Élimination des mots chaines de caractères vides  
\end{enumerate}

Nous avons nommé cette nouvelle méthode de prétraitement des données : \texttt{Processing Reference Réarrangé}.
Voici les statistiques de notre collection après cette nouvelle méthode de prétraitement des données:

\begin{table}[!h]
  \begin{minipage}{0.35\linewidth}
      \centering
      \resizebox{\textwidth}{!}{%
          \begin{tabular}{|c|c|c|}
              \hline
              \textbf{Statistiques} & \textbf{Valeur} \\
              \hline
              Taille moyenne des documents &  628.267 \\
              Taille moyenne des mots &  9.227 \\
              Nombre de mots unique &  226,564 \\
              Nombre de mots total &  6.15953e+06 \\
              \hline
          \end{tabular}%
      }
      \caption{Statistiques de la collection}
  \end{minipage}%
  \hspace{0.05\linewidth} % Add a 5% width margin
  \begin{minipage}{0.60\linewidth}
    Nous pouvons tout de suite remarquer que la taille moyenne des documents a légèrement diminué, 
    et que la taille moyenne des mots a légèrement augmenté. Le vocabulaire de notre collection a également
    augmenté. De plus nous avons un total de mots légèrement inférieur à la première méthode de prétraitement des données, 
    environ 25,000 mots de moins.    
  \end{minipage}
\end{table}

Pour évaluer l'amplitude de l'impact de cette nouvelle méthode de prétraitement des données, 
nous avons décidé d'observer la fréquence des mots de nos requêtes avant et après cette nouvelle méthode.

\begin{table}[!h]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
            \hline
            \textbf{} & \textbf{actor} & \textbf{algorithm} & \textbf{analysi} &
            \textbf{benefit} & \textbf{exclus} & \textbf{film} & \textbf{health} &
            \textbf{hill} & \textbf{inform} & \textbf{learn} & \textbf{link} & 
            \textbf{machin} \\
            \hline
            \textbf{Processing Reference} & 7086 & 35062 & 8085 & 4671 & 1888 & 16595 & 11391 & 4513 & 17440 & 6003 & 12090 & 9122 \\
            \textbf{Processing Reference Réarrangé} & 7113 & 35326 & 8164 & 4716 & 1891 & 16708 & 11411 & 4526 & 0 & 6028 & 12306 & 9160 \\
            \textbf{Différence} & 27 & 264 & 79 & 45 & 3 & 113 & 20 & 13 & -17440 & 25 & 216 & 38 \\
            \hline
        \end{tabular}%
    }
    \caption{Fréquence des mots des requêtes (Partie 1)}
\end{table}

\begin{table}[!h]
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
            \hline
            \textbf{} & \textbf{model} & \textbf{mutual} & \textbf{network} &
            \textbf{oil} & \textbf{oliv} & \textbf{oper} & \textbf{probabilist} & 
            \textbf{rank} & \textbf{retriev} & \textbf{score} & \textbf{supervis} &
            \textbf{system} & \textbf{web} \\
            \hline
            \textbf{Processing Reference} & 15337 & 2708 & 17820 & 14270 & 3023 & 16591 & 1230 & 3643 & 13560 & 4179 & 588 & 38683 & 20128 \\
            \textbf{Processing Reference Réarrangé} & 15422 & 2711 & 17968 & 14409 & 3027 & 16622 & 1229 & 3680 & 13570 & 4198 & 589 & 39351 & 20188 \\
            \textbf{Différence} & 85 & 3 & 148 & 139 & 4 & 31 & -1 & 37 & 10 & 19 & 1 & 668 & 60 \\
            \hline
        \end{tabular}%
    }
    \caption{Fréquence des mots des requêtes (Partie 2)}
\end{table}
Nous avons au total -15393 mots sur les mots de la requête. Nous remarquons immédiatement que le mot "information" (inform) à disparu de notre collection.
Cela est dû à l'étape de suppression des mots vides (stopwords), qui a éliminé le mot "information" de notre collection.
Ceci confirme que nos étapes de prétraitement sont bien réalisées 

Après avoir retiré le mot "information" de notre list de mots vides (stopwords), 
nous avons un total de 2119 mots en plus sur les mots de la requête.

Voici une nouvelle fois les statistiques de notre collection après cette nouvelle méthode de prétraitement :

\begin{table}[!h]
  \begin{minipage}{0.35\linewidth}
      \centering
      \resizebox{\textwidth}{!}{%
          \begin{tabular}{|c|c|c|}
              \hline
              \textbf{Statistiques} & \textbf{Valeur} \\
              \hline
              Taille moyenne des documents &  650.929  \\
              Taille moyenne des mots & 8.42026 \\
              Nombre de mots unique &  211,770 \\
              Nombre de mots total &  6.3817e+06 \\
              \hline
          \end{tabular}%
      }
      \caption{Statistiques de la collection}
  \end{minipage}%
  \hspace{0.05\linewidth} % Add a 5% width margin
  \begin{minipage}{0.60\linewidth}
    Nous récupérons maintenant environs 150,000 mots en plus avec une taille moyenne des mots légèrement inférieure.    
    En théorie, nous devrions obtenir de meilleurs résultats avec cette nouvelle méthode, donc en ce basant sur cette
    hypothèse, nous avons décider d'améliorer encore un peu plus notre méthode de prétraitement des données.
  \end{minipage}

\end{table}

\subsubsection{Optimisation du Prétraitement des Données : Expérimentations avec l'Élimination des Mots Rares}
Nous avions remarqué que le vocabulaire de notre collection était très grand, 
nous avions environs 200,000 mots uniques. Cependant, en observant les mots de notre vocabulaire, 
nous nous sommes rendu compte que beaucoup de mots avait une fréquence très faible ou ne possédaient 
aucun sens. Nous avons donc décidé de créer une liste de mots que nous avons nommé \texttt{outliers},
qui contient les mots qui apparaissent moins de 5 fois dans notre collection et les mots ayant 
une longueur inférieure à 3 caractères. Nous avons ensuite éliminé ces mots de notre collection.


Voici les statistiques de notre collection après cette nouvelle méthode de prétraitement des données:

\begin{table}[!h]
  \begin{minipage}{0.35\linewidth}
      \centering
      \resizebox{\textwidth}{!}{%
          \begin{tabular}{|c|c|c|}
              \hline
              \textbf{Statistiques} & \textbf{Valeur} \\
              \hline
              Taille moyenne des documents &  580.832  \\
              Taille moyenne des mots & 6.68121 \\
              Nombre de mots unique &  40,763 \\
              Nombre de mots total &  5.69448e+06 \\
              \hline
          \end{tabular}%
      }
      \caption{Statistiques de la collection}
  \end{minipage}%
  \hspace{0.05\linewidth} % Add a 5% width margin
  \begin{minipage}{0.60\linewidth}
    A l'aide de cette nouvelle méthode notre vocabulaire est maintenant constitué de 40,763 mots uniques,
    avec une taille moyenne des mots de 6.68121. Nous avons donc réduit notre vocabulaire de plus de 150,000 mots.
    Nous avons également une taille moyenne des documents de 580.832 mots, ce qui est une réduction de 70 mots par rapport à la méthode précédente.
  \end{minipage}
\end{table}

\section{Méthodes et Paramétrages}

Nous avons utilisé plusieurs méthodes et modèles pour la recherche d'information et avons 
réalisé plusieurs expérimentations pour essayé de trouver la meilleure configuration possible. Nous présentons
dans cette section les différentes méthodes et modèles que nous avons utilisé, ainsi que les paramétrages que nous avons utilisé, 
et les résultats obtenus.

\subsection{Méthodes de Recherche d'Information}

Tout au long de ce projet, nous avons recherché à trouver et optimiser la meilleure méthode de recherche d'information. N'ayant pas la capacité d'évaluer de 
notre côter nos modèles, nous avons mis en place quelque outils pour nous permettre de comparer des runs entre elles.

Nous avons notamment utilisé des mesures de corrélation pour comparer les résultats obtenus, ainsi nous étions en mesures 
de savoir si des changements dans le code modifiaient les runs produites. Cette mesure de corrélations ne nous à pas permis d'évaluer
nos modèles, mais nous à permis de comparer une run "référérence" en l'occurrence une run baseline, avec d'autres runs pour voir si les changements
celles ci avait un classement similaires ou pas.

En ce qui concerne les runs éléments, nous avons calculé le nombre de balises ciblées retourné dans les runs. Ainsi, si une balise ciblée était
très peu présente dans les runs, nous pouvions en déduire que cette balise ne comportait pas beaucoup d'information, et que nous devions peut-être
la retirer de notre liste de balises ciblées.

\subsubsection{Méthodes de Recherche d'Information : Smart LTN}

Le poids TF-IDF (\textit{Term Frequency-Inverse Document Frequency}) d'un terme dans un document est déterminé par la formule suivante :

\begin{equation}
\text{TF-IDF}(i, d) = \left(1 + \log_{10}(\text{TF}(i, d))\right) \times \text{IDF}(i)
\end{equation}

où :
\begin{itemize}
    \item $\text{TF}(i, d)$ représente la fréquence du terme $i$ dans le document $d$.
    \item $\text{IDF}(i)$ est la fréquence inverse du document pour le terme $i$.
\end{itemize}

Les composants individuels sont calculés de la manière suivante :

\begin{equation}
\text{IDF}(i) = \log_{10}\left(\frac{\text{Taille de la Collection}}{\text{Fréquence du Terme}(i)}\right)
\end{equation}

\begin{equation}
\text{TF}(i, d) = \text{Fréquence du Terme}(i, d)
\end{equation}

Dans le cadre de notre système, l'$\text{IDF}$ et le $\text{TF}$ sont calculés en tenant compte des balises ciblées.
Ci-dessous quelques résultats obtenus sur notre run baseline.
\begin{table}[h]
    \centering
    \begin{tabular}{l c c}
        \toprule
        \textbf{Run Name} & \textbf{MAGP Value} & \textbf{P[0, 1]} \\
        \midrule
        \dots\_5\_ltn\_article\_stop670\_porter & 0.1687 & 0.3835 \\
        \dots\_8\_ltn\_article\_stop670\_nostem & 0.1699 & 0.3996 \\
        \dots\_11\_ltn\_article\_nostop\_porter & 0.1515 & 0.3265 \\
        \dots\_14\_ltn\_article\_nostop\_nostem & 0.1699 & 0.3996 \\
        \bottomrule
    \end{tabular}
    \caption{Résultats de la méthode Smart LTN}
    \label{tab:result_ltn}
\end{table}

Nous pouvons remarquer que le fait de ne pas éliminer les mots vides (stopwords) 
a un impact négatif sur les résultats obtenus. En effet, les runs 8 et 14 ont obtenu les meilleurs résultats,
avec un score de MaGP de 0.1699, et un P[0, 1] de 0.3996. Nous avons également remarqué que la racinisation des mots (stemming)
n'a pas d'impact significatif sur les résultats obtenus. En effet, les runs 8 et 14 ont obtenu les mêmes résultats,
alors que le run 8 a été réalisé avec la racinisation des mots, et le run 14 sans la racinisation des mots.


\subsubsection{Méthodes de Recherche d'Information : Smart LTC}
Le schéma de pondération LTC (Logarithmique-Term Frequency and Cosine Normalization) optimise la précision des recherches en ajustant les poids des termes selon leur fréquence et la longueur des documents. Voici les étapes clés de notre implémentation :

\begin{enumerate}
\item \textbf{Calcul du Poids LTC} : Utilise la fréquence logarithmique du terme (L), la fréquence inverse du document (T), et la normalisation cosinus (C) pour équilibrer l'importance des termes entre documents de longueurs variées.

\item \textbf{Fréquence Logarithmique du Terme (L)} :
\begin{equation}
L(i, d) = 1 + \log_{10}(\text{TF}(i, d))
\end{equation}
$\text{TF}(i, d)$ représente la fréquence du terme $i$ dans le document $d$.

\item \textbf{Fréquence Inverse du Document (T)} :
\begin{equation}
T(i) = \log_{10}\left(\frac{N}{\text{DF}(i)}\right)
\end{equation}
$N$ est le nombre total de documents, et $\text{DF}(i)$ le nombre de documents contenant le terme $i$.

\item \textbf{Normalisation Cosinus (C)} :
\begin{equation}
C(d) = \frac{1}{\sqrt{\sum_{i \in d} (L(i, d) \times T(i))^2}}
\end{equation}
\item \textbf{Poids Final LTC} :
\begin{equation}
\text{LTC}(i, d) = (L(i, d) \times T(i)) \times C(d)
\end{equation}
\end{enumerate}

L'implémentation de ce schéma a permis à notre système une évaluation équilibrée des termes en fonction de leur fréquence et de la longueur des documents, améliorant la pertinence des résultats de recherche.
\begin{table}[h]
    \centering
    \begin{tabular}{l c c}
        \toprule
        \textbf{Run Name} & \textbf{MAGP Value} & \textbf{P[0, 1]} \\
        \midrule
        BengezzouIdrissMezianeGhilas\_6\_ltc\_article\_stop670\_porter & 0.0834 & 0.22818 \\
        BengezzouIdrissMezianeGhilas\_9\_ltc\_article\_stop670\_nostem & 0.0699& 0.1987 \\
        BengezzouIdrissMezianeGhilas\_12\_ltc\_article\_nostop\_porter & 0.0668 & 0.1930 \\
        BengezzouIdrissMezianeGhilas\_15\_ltc\_article\_nostop\_nostem & 0.0699 & 0.1987 \\
        \bottomrule
    \end{tabular}
    \caption{Résultats de la méthode Smart LTC}
    \label{tab:result_ltn}
\end{table}

\subsubsection{Méthodes de Recherche d'Information : Smart LNU}
Le schéma de pondération LNU (Logarithmic Normalization and Unique-term adjustment) ajuste le poids des termes en fonction de leur fréquence, de la longueur du document, et du nombre de termes distincts. Le processus est résumé comme suit :

\begin{enumerate}
    \item \textbf{Fréquence Logarithmique Normalisée (LNN)} : 
    \begin{equation}
    \text{LNN}(i, d) = 1 + \log_{10}(\text{TF}(i, d))
    \end{equation}
    où \(\text{TF}(i, d)\) est la fréquence du terme \(i\) dans le document \(d\).
    
    \item \textbf{Normalisation par la Longueur} : 
    \begin{equation}
    \text{LN}(d) = 1 + \log_{10}\left(\frac{\text{Length}(d)}{\text{AVDL}}\right)
    \end{equation}
    ajuste le poids selon la longueur du document \(d\) par rapport à la longueur moyenne des documents \(\text{AVDL}\).
    
    \item \textbf{Ajustement Basé sur les Termes Distincts} : 
    \begin{equation}
    \text{Adjustement}(d) = (1 - \text{slope}) + \text{slope} \times \frac{\text{NTD}(d)}{\text{Pivot}}
    \end{equation}
    où \(\text{NTD}(d)\) est le nombre de termes distincts dans \(d\).
    
    \item \textbf{Poids Final} : 
    \begin{equation}
    \text{Weight}(i, d) = \frac{\text{LNN}(i, d)}{\text{LN}(d)} / \text{Adjustement}(d)
    \end{equation}
    combinant les éléments ci-dessus.
\end{enumerate}
Voici les scores MaGP que nous avons obtenu en essayant de faire varier le slope de 0.1 à 1.1 
[\ref{fig:lnu_results}].

Comme nous pouvons le remarquer ici, le MaGP est bien loin des scores que nous avons pu atteindre en utilisant d'autres modèle de pondération. Après révision du code, nous nous sommes aperçus d'une erreur au niveau du calcul de l'ajustement dans la formule LNU. 
Les poids étaient calculés de la manière suivante :
\begin{center}
\texttt{adjustement = (1 - self.slope) + pivot + (self.slope * nt\_d) \\
weight = (lnn / length\_normalization) / adjustement}
\end{center} 
Cette erreur a conduit à un calcul inexact de l'ajustement, impactant directement le poids final attribué à chaque terme dans les documents, et cette mauvaise estimation a affecté la capacité du modèle à distinguer efficacement les documents pertinents des non pertinents, ce qui se reflète dans une baisse de la précision moyenne (MaGP)


\newpage


\newpage

\subsubsection{Méthodes de Recherche d'Information : BM25}

La méthode de pondération BM25, également connue sous le nom de Best Matching 25,
est une technique couramment utilisée en recherche d'information pour attribuer des poids aux termes dans un index inversé.
Cette méthode prend en compte la fréquence du terme dans un document, la fréquence du terme dans l'ensemble de la collection,
et la longueur du document.

Le calcul du poids BM25 pour un terme dans un document est défini par la formule suivante :

\begin{equation}
\text{BM25}(i, d) = \frac{{\text{TF} \times (\text{k1} + 1)}}{{\text{k1} \times \left((1 - \text{b}) + \text{b} \times \left(\frac{\text{DL}}{\text{AVDL}}\right)\right) + \text{TF}}} \times \text{IDF}
\end{equation}

où :
\begin{itemize}
    \item $\text{TF}$ est la fréquence du terme dans le document.
    \item $\text{DL}$ est la longueur du document.
    \item $\text{AVDL}$ est la longueur moyenne des documents dans la collection.
    \item $\text{IDF}$ est la fréquence inverse du document pour le terme.
    \item $\text{k1}$ et $\text{b}$ sont des paramètres de réglage qui contrôlent l'impact de la fréquence du terme et de la longueur du document, respectivement. \\
\end{itemize}

La mise en œuvre de la méthode BM25 dans notre système de recherche d'information est réalisée par la  classe \texttt{BM25Weighting}, héritant de la classe abstraite \texttt{WeightingStrategy}. Cette classe propose une méthode \texttt{calculate\_weight} 
qui construit l'index inversé pondéré en utilisant la pondération BM25. 
Les paramètres $\text{k1}$ et $\text{b}$ sont configurables lors de l'instanciation de la classe \texttt{BM25Weighting}, pour permettre d'expérimenter avec différentes valeurs de ces paramètres.

Les résultats obtenus avec cette méthode seront présentés dans la section suivante,
soulignant son impact sur les performances du système de recherche d'information.

\begin{table}[h]
  \centering
  \begin{tabular}{l c c}
      \toprule
      \textbf{Run Name} & \textbf{MAGP Value} & \textbf{P[0, 1]} \\
      \midrule
      \dots\_7\_bm25\_article\_stop670\_porter\_k1\_b0.5 & 0.2368 & 0.5157 \\
      \dots\_10\_bm25\_article\_stop670\_nostem\_k1\_b0.5 & 0.2364 & 0.5634 \\
      \dots\_13\_bm25\_article\_nostop\_porter\_k1\_b0.5 & 0.1924 & 0.4325 \\
      \dots\_16\_bm25\_article\_nostop\_nostem\_k1\_b0.5 & 0.2332 & 0.5537 \\
      \bottomrule
  \end{tabular}
  \caption{Résultats de la méthode BM25}
  \label{tab:result_bm25}
\end{table}

Nous remarquons encore une fois que les runs ayant les meilleurs résultats sont ceux qui ont été réalisés 
avec la racinisation des mots (stemming), et sans l'élimination des mots vides (stopwords).

De plus, en comparaisons avec les résultats obtenus avec la méthode Smart LTN et la méthode Smart LTC, BM25 
a obtenu les meilleurs résultats, avec un score de MaGP de 0.2368 sur la run 7, et un P[0, 1] de 0.5634 sur la run 10.
 

\begin{table}[!h]
    \begin{minipage}{0.55\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{image.png}
        \label{fig:enter-label}
        \caption{Evolution de MaGP en fonction de k1 et b}
    \end{minipage}%
    \hspace{0.05\linewidth} % Add a 5% width margin
    \begin{minipage}{0.45\linewidth}
        Nous pouvons remarquer que notre score de MaGP est maximisé pour des valeurs de $\text{b}$ 
        assez faible (entre 0.5 et 0.65) et des valeurs de $\text{k1}$ entre 1 et 1.1. \\

        Nous pouvons remarquer très clairement sur le graphique que notre score de MaGP est maximisé avec une valeur de $\text{b}$ 
        égale à 0.5. En effet, sur notre interval de recherche pour $\text{k1}$, nous avons un score élevé et assez stable 
        pour des valeurs de $\text{k1}$ entre 1 et 1.4.
    \end{minipage}
\end{table}

Nous avons également voulu observer si les valeurs de $\text{k1}$ et $\text{b}$ qui maximisent notre score de MaGP
maximisent également notre score de P[0, 1]. 
Voici les résultats obtenus :

\begin{table}[!h]
    \begin{minipage}{0.45\linewidth}
        Nous remarquons tout de suite que les valeurs maximisant notre score de MaGP ne maximisent pas notre score de P[0, 1].
        En effet, nous avons un score de P[0, 1] maximisé pour des valeurs de $\text{b}$ entre 0.65 et 0.75,
        et des valeurs de $\text{k1}$ entre 1.0 et 1.3. \\

        Nous savons donc que les valeurs permettant de maximiser notre score de MaGP ne maximisent pas notre score de P[0, 1].
    \end{minipage}
    \hspace{0.05\linewidth} % Add a 5% width margin
    \begin{minipage}{0.55\linewidth}
        \centering
        \includegraphics[width=1\linewidth]{heatmap_p_bm25.png}
        \label{fig:enter-label}
        \caption{Evolution de P[0, 1] en fonction de k1 et b}
    \end{minipage}%
\end{table}

Voir \ref{fig:rap_prec_best_art_bm25} la courbe de rappel-précision pour notre meilleur run article avec BM25.



En nous concentrant sur la courbe de rappel-précision,
nous pouvons observer que notre modèle BM25 a une précision assez faible pour des valeurs de rappel assez élevées.

Nous avons par la suite essayé d'évaluer notre modèle BM25 en essayant de cibler les balises.
En ciblant uniquement la balise \texttt{body}, nous
avons obtenu notre meilleur score de MaGP, avec un score de 0.2512 et un P[0, 1] de 0.5136.

\newpage

Concernant les résultats obtenus avec la méthode BM25 en ciblant les balises, nous avons obtenu les meilleurs résultats en 
associant systématiquement la balise \texttt{body} ou \texttt{article} avec d'autres balises plus spécifiques, comme \texttt{header} ou \texttt{section}.

\begin{table}[!h]
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{|c|c|c|c|c|}
      \hline
      \textbf{Run Id} & \textbf{Granularity} & \textbf{MAPGP Score} & \textbf{P[0, 1] Score} \\
      \hline
      92 & bdy, header & 0.1685 & 0.3731 \\
      88 & article, event, title & 0.1667 & 0.3299 \\
      18 & article, header & 0.1621 & 0.3711 \\
      20 & article, sec & 0.1499 & 0.3817 \\
      93 & bdy, header, sec & 0.1454 & 0.3708 \\
      \hline
  \end{tabular}%
  }
  \caption{Résultats de la méthode BM25 avec ciblage de balises}
\end{table}

De plus, lors de nos expérimentations nous avons essayé deux approche différentes pour le 
calcul du DF (Document Frequency). 
Nous avons calculé le DF en prenant en compte les chemins des balises, et en ne prenant pas en compte les chemins des balises.
Nous avons constaté que le fait de ne pas prendre en compte les chemins des balises a un impact très négatif sur les résultats obtenus.
En effet, une simple run avec la balise \texttt{body} nous a donné un 
score de MaGP de 0,0752 et une precision de 0,3379 (voir run \texttt{6\_bm25\_bdy\_stop671\_porter\_k1\_b0.5} practice 5v4).

En ne prenant pas en compte les chemins des balises, nous sommes revenus à des run 
\texttt{body} et \texttt{article} très similaires.


\subsubsection{Méthodes de Recherche d'Information : BM25F_w}

La méthode de pondération $BM25F_w$ étend la méthode BM25 traditionnelle en intégrant une approche pondérée des champs pour améliorer la précision de la recherche d'information. Cette technique est particulièrement efficace pour les documents structurés, où les termes peuvent avoir une importance différente selon leur contexte dans le document.

La formule de base pour le calcul du poids $BM25F_w$ pour un terme dans un document multi-champs est :

\begin{equation}
\text{$BM25F_w$}(i, d) = \sum_{f \in F} w_f \times \frac{\text{TF}_{f} \times (\text{k1} + 1)}{\text{k1} \times \left((1 - \text{b}) + \text{b} \times \left(\frac{\text{DL}_f}{\text{AVDL}_f}\right)\right) + \text{TF}_f} \times \text{IDF}
\end{equation}

La différence principale réside dans l'introduction de poids de champ ($w_f$) dans BM25F\_w, permettant d'ajuster l'importance relative de chaque champ dans le calcul de la pertinence. Cette approche permet à BM25F\_w de reconnaître que certains termes peuvent avoir plus de signification lorsqu'ils apparaissent dans certaines parties du document, comme le titre, par rapport à leur présence dans le corps du texte.

Nous avons évalué les performances de notre modèle BM25F\_w en utilisant différentes combinaisons des paramètres \(\alpha\), \(\beta\), et \(\gamma\), avec \(k1=1.2\) et \(b=0.75\). 

\begin{figure}[H]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_BM25FW.png}
        \caption{Résultats MaGP BM25FW sous différents paramètres}
        \label{fig:magp_bm25fw}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{results_PRECISION_BM25FW.png}
        \caption{Résultats de précision P[0,1] pour BM25FW sous différents paramètres}
        \label{fig:precision_bm25fw}
    \end{minipage}
\end{figure}

\textbf{Analyse des résultats \\}
Concernant la Mean Average Precision (MaGP), nous avons constaté des variations similaires en fonction des paramètres \(\alpha\), \(\beta\), et \(\gamma\). Les configurations initiales \((\alpha=1.0, \beta=1.0)\) ont montré une performance robuste indépendamment de la valeur de \(\gamma\), tandis que l'augmentation de \(\alpha\) et \(\beta\) a conduit à une diminution de MaGP, particulièrement pour \((\alpha=1.5, \beta=1.5, \gamma=1.0)\).

Les meilleures performances en termes de précision P[0,1] ont été observées pour les configurations \((\alpha=1.0, \beta=1.0, \gamma=1.0)\) et \((\alpha=1.0, \beta=1.0, \gamma=1.5)\), avec une précision de \(0.22\). La diminution de la précision pour des valeurs plus élevées de \(\beta\) et \(\gamma\) suggère que surpondérer ces champs par rapport à \(\alpha\) n'améliore pas la qualité des premiers résultats de recherche. La précision la plus faible a été enregistrée pour la combinaison \((\alpha=1.5, \beta=1.5, \gamma=1.0)\), soulignant l'importance d'un équilibre approprié entre les paramètres pour maximiser l'efficacité du modèle. 
Dans notre cas, l'application du modèle BM25F\_w sur des balises d'articles a produit un Mean Average Precision (MaGP) différent de celui observé lors de l'utilisation du modèle BM25 classique. Cette différence suggère que le choix des balises sur lesquelles le modèle est appliqué inluence de manière significative les résultats obtenus, car BM25F\_w est conçu pour tirer parti de la structure des documents en attribuant des poids différents aux termes selon leur emplacement dans différents champs ou balises du document.

\newpage


\subsubsection{Méthodes de Recherche d'Information : BM25F_r}
Dans le cadre de ce projet nous avons aussi eu l'occasion d'implémenter une autre méthode de pondération: $BM25F_r$. La méthode de pondération $BM25F_r$, proposée par Robertson, est une variante du modèle BM25 classique qui est spécifiquement conçue pour améliorer la recherche d'informations dans des documents structurés. Tout comme $BM25F_w$, $BM25F_r$ étend le modèle BM25 en prenant en compte la structure des documents, mais avec une approche différente focalisée sur la représentation et la pondération des termes en fonction de leur pertinence dans divers champs du document.

Contrairement à BM25F\_w, qui ajuste l'importance des champs dans le calcul de la pertinence à travers des poids statiques \(w_f\), BM25F\_r adopte une approche de pondération dynamique. Cette méthode ajuste la contribution de chaque champ en fonction de ses caractéristiques propres, utilisant des facteurs de normalisation et de pondération variables. Cela permet une évaluation plus précise et adaptative de la pertinence des termes selon le contexte spécifique de chaque champ, suivant les principes énoncés par Robertson.
La formule de pondération pour BM25F\_r est représentée comme suit :

\begin{equation}
\text{$BM25F_r$}(t, D) = IDF(t) \times \sum_{f \in F} \left( \frac{(k_1 + 1) \times TF_{f, t, D}}{K_{f, D} + TF_{f, t, D}} \right) \times W_f
\end{equation}

Ci-dessous les résultats obtenus par nos runs $BM25F_r$:
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{results_BM25FR.png}
    \caption{MaGP and Precision - BM25F_r}
    \label{fig:enter-label}
\end{figure}

Encore une fois, nos résultat sont très éloignés d'une run BM25 "classique" pour des paramètres à 1. Cela est très probablement du au choix des balises pour réaliser nos runs, car en effet, 
en effectuant des tests sur des collection beaucoup plus petite nous obtenions des résultats identiques à ceux de BM25.
Malheureusement, nous n'avons pas eu le temps de réaliser des expérimentations plus poussées pour sélectionner les bonnes balises. Cependant, nous avons quand même
pu observer qu'en sélectionnant les balises \texttt{body} et \texttt{article} nous obtenions les même résultats que pour BM25.

\newpage


\subsubsection{Méthodes de Recherche d'Information : DeepLearning}

Dans cette section, nous explorons l'utilisation de méthodes de recherche d'information basées
sur le Deep Learning. Notre démarche s'est concentrée sur l'exploration de modèles pré-entraînés
et l'évaluation de leur performance dans le contexte de la recherche d'information (IR).

Nous avons entrepris des recherches approfondies dans la littérature liée à l'IR et au Deep Learning,
identifiant de nombreuses ressources pertinentes.
Souhaitant évaluer ces méthodes, nous avons choisi d'effectuer plusieurs runs, en utilisant 
des modèles pré-entraînés disponibles.

Parmi les documents que nous avons trouvés particulièrement intéressants, nous mettons en avant le papier 
intitulé "BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models" 
(\href{https://openreview.net/forum?id=wCu6T5xFjeJ}{https://openreview.net}). 
Ce document propose un benchmark de plusieurs modèles sur divers ensembles de données. 
De plus, les auteurs ont rendu leur code accessible sur \href{https://github.com/beir-cellar/beir}{Github}.

Pour nos expérimentations, nous avons utilisé le code mis à disposition par les auteurs et généré des runs pour évaluer les performances de ces modèles sur notre collection de documents.
\begin{itemize}
    \item \textbf{msmarco-distilbert-base-tas-b} : Utilisation de DistilBERT avec une fonction de similarité cosinus. Les embeddings de la requête et du document sont générés dans le même espace vectoriel.
    \item \textbf{msmarco-roberta-base-ance-firstp} : Utilisation de Sentence-BERT avec une fonction de produit scalaire. Intègre ANCE, une approximation d'ANN (Approximate Nearest Neighbors), pour les embeddings multilingues.
    \item \textbf{facebook/dpr-question\_encoder-multiset-base} : Utilisation de DPR (Deep Passage Retrieval) avec des encodeurs de question et de contexte. La fonction de score est basée sur le produit scalaire pour la récupération dense de passages. \\
\end{itemize}


La librairie BEIR fournit d'autres modèles et techniques, telles que le Re-ranking, qui permet de réordonner les résultats d'une recherche initiale en utilisant un modèle de Deep Learning. Les auteurs proposent 5 techniques différentes : \\
\begin{itemize}
    \item \textbf{Lexical Retrieval :} Avec un BM25 associé à ElasticSearch ou Anserini.
    \item \textbf{Sparse Retrieval :} Utilisation de modèles DeepCT, SPARTA, et DocT5query.
    \item \textbf{Dense Retrieval :} Utilisation de modèles DPR, ANCE, TAS-B, et GenQ.
    \item \textbf{Late-Interaction :} Utilisation de ColBERT pour l'agrégation des interactions tardives.
    \item \textbf{Re-Ranking Model :} Utilisation de modèles comme BM25 + CE, qui réordonne les résultats d'une recherche initiale avec un modèle de distillation des connaissances. \\
\end{itemize}

Les résultats obtenus avec ces modèles était très faible sur notre collection. Il est possible que les transformers utilisés pour ces modèles ne soient pas adaptés à notre collection de documents. 


\newpage


\section{Performances \& Optimisations}

Dans cette section, nous présentons les efforts déployés pour améliorer les performances et optimiser le code, réduisant ainsi les temps d'exécution.
Nous avons adopté les principes de la Programmation Orientée Objet (POO) pour structurer notre code de manière plus efficace.

En ce qui concerne les structures de données, nous avons entrepris des optimisations sur des éléments clés tels que l'index inversé, la fréquence des termes,
la fréquence des documents, etc. L'objectif principal était d'optimiser ces structures pour favoriser les accès directs aux données,
atteignant ainsi une complexité temporelle de O(1) dans de nombreux cas.

Par ailleurs, afin d'optimiser l'efficacité des recherches, nous avons suivi les conseils donnés en classe. Une des approches adoptées a été de n'indexer 
que les mots présent dans les requêtes, réduisant ainsi la taille de l'index et améliorant les performances globales du système. Ces ajustements ont été réalisés 
dans le but d'obtenir des temps d'exécution plus rapides et une utilisation plus efficace des ressources.


\section{Conclusion}
Ce projet nous a permis de mettre en pratique les connaissances acquises en cours de Recherche d'Information. Nous avons exploré diverses méthodes et paramétrages pour optimiser un système de recherche d'information sur un corpus de documents. Notre travail s'est concentré sur l'implémentation de modèles basés sur les pondérations TF-IDF, BM25, BM25F_w, et BM25F_r, ainsi que sur l'expérimentation avec des techniques de prétraitement des données pour améliorer les performances.

Les résultats obtenus ont montré l'importance cruciale du prétraitement des données et du choix judicieux des paramètres pour les modèles de pondération. Nous avons constaté que des ajustements fins des paramètres, comme ceux de BM25 (\(k1\) et \(b\)), ainsi que la sélection stratégique des balises pour BM25F_w et BM25F_r, peuvent avoir un impact significatif sur la qualité des résultats de recherche. L'erreur détectée dans le calcul de l'ajustement pour BM25F_r souligne également l'importance d'une vérification rigoureuse du code pour garantir l'exactitude des implémentations.

En outre, nos expérimentations avec BM25F_w et BM25F_r ont révélé que la structure des documents et le choix des balises ciblées jouent un rôle déterminant dans la performance des systèmes de recherche d'information. Il est apparu que l'application de ces modèles sur des balises spécifiques peut influencer de manière significative les résultats, soulignant l'importance de comprendre la structure des documents pour une indexation et une recherche efficaces.

En conclusion, ce projet a non seulement renforcé notre compréhension des principes fondamentaux de la recherche d'information mais a également mis en lumière les défis associés à l'optimisation des systèmes de recherche d'information. Les leçons apprises ici, notamment l'importance du prétraitement, le choix des paramètres, et la compréhension de la structure des documents, seront précieuses pour nos futurs travaux dans le domaine.

Pour les perspectives futures, nous envisageons d'explorer l'intégration de techniques de Deep Learning pour la recherche d'information, ainsi que l'expansion de notre système pour traiter des corpus de documents plus divers et plus vastes. Il serait également intéressant de développer des interfaces utilisateur intuitives pour permettre aux utilisateurs d'interagir plus facilement avec le système de recherche d'information.

\newpage

\section{Annexes}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{LNU_magp_results.png}
    \caption{MaGP pour LNU en fonction de différents slopes}
    \label{fig:lnu_results}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{recall_precision_bm25_baseline.png}
    \caption{Courbe de rappel-précision pour BM25}
    \label{fig:rap_prec_best_art_bm25}
\end{figure}

\end{document} 