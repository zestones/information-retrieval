Correction de bugs:
----------------
- Correction de bugs concernant le calcul des stats de la collection :
    Nous avons remarqué que le calcul des stats (donc du dl, avdl et N) n'était pas correcte pour les runs éléments.

- Correction de bugs concernant bm25fr:
    Après la combinaison du tf, nous ne recalculions pas les dl donc les résultats n'étaient pas correcte.


Quelque expérimentations:
-------------------------
- Concernant la différence entre une run "article" et une run "bdy" que nous avions évoqué dans le rapport précédent :
    Après avoir fait des tests, nous avons remarqué que le calcul de notre df prenait en compte les xpath, nous avons donc 
    voulu essayer de calculer un df sans tenir compte des xpath. Ce qui nous a permis d'obtenir des résultats plus cohérents, puisque 
    une run "article" et une run "bdy" ont des résultats très proches maintenant. 
    Les résultats des runs éléments qui ont été faites jusque la avait toute été faite avec le calcul de df qui prenait en compte les xpath c'est pourquoi nos résultats de run éléments étaient si mauvais.
    Toute les expérimentations que nous avons faites par la suite ont été faites avec ce nouveau calcul de df (sans les xpath).



---- 
EXPERIMENTATION:
---------------

Ancienne method article:
------------------------
preprocess Reference:
- bm25F (~2) 1bm25FW + 1bm25FR
- lnu (~1)

preprocess Reference réarangé:
- bm25 (~1)

Nouvelle Method:
---------------
Processing Reference 
- bm25 (~1)
- bm25F (~2)
 
Processing Reference réarangé:
- bm25 (~1)

Processing With new steps:
- baseline (~12)
- element (~10)
- gridSearch (~50)

----------------

stats de la collection sur ancienne methode avec preprocess reference:
╒═════════════════════════╤═════════════╕
│ Statistic               │    Value    │
╞═════════════════════════╪═════════════╡
│ Average document length │   654.008   │
├─────────────────────────┼─────────────┤
│ Average word length     │   8.39066   │
├─────────────────────────┼─────────────┤
│ Total unique words      │   210232    │
├─────────────────────────┼─────────────┤
│ Total words             │ 6.41189e+06 │
╘═════════════════════════╧═════════════╛

Stats de la collection sur nouvelle methode avec processing reference rearrangé:
╒═════════════════════════╤═════════════╕
│ Statistic               │    Value    │
╞═════════════════════════╪═════════════╡
│ Average document length │   628.267   │
├─────────────────────────┼─────────────┤
│ Average word length     │   9.22707   │
├─────────────────────────┼─────────────┤
│ Total unique words      │   226564    │
├─────────────────────────┼─────────────┤
│ Total words             │ 6.15953e+06 │
╘═════════════════════════╧═════════════╛
Du fait de la supression des stopwords avant toute modif des mots, on a moins de mots dans la collection
~250000 mots en moins.

En observant comment ont évolué les mots de la requete avec la nouvelle methode:

                        actor    algorithm    analysi    benefit    exclus    film    health    hill    inform    learn    link    machin    model    mutual    network    oil    oliv    oper    probabilist    rank    retriev    score    supervis    system    web
--------------------  -------  -----------  ---------  ---------  --------  ------  --------  ------  --------  -------  ------  --------  -------  --------  ---------  -----  ------  ------  -------------  ------  ---------  -------  ----------  --------  -----
Original method          7086        35062       8085       4671      1888   16595     11391    4513     17440     6003   12090      9122    15337      2708      17820  14270    3023   16591           1230    3643      13560     4179         588     38683  20128
New method               7113        35326       8164       4716      1891   16708     11411    4526         0     6028   12306      9160    15422      2711      17968  14409    3027   16622           1229    3680      13570     4198         589     39351  20188
Frequency difference       27          264         79         45         3     113        20      13    -17440       25     216        38       85         3        148    139       4      31             -1      37         10       19           1       668     60
----------------------------------------
Overall difference: -15393

?? Information a disparu vérification de pourquoi: :
le mot est présent dans la liste des stopwords 

Apres supression du mots information des stopwords:
                        actor    algorithm    analysi    benefit    exclus    film    health    hill    inform    learn    link    machin    model    mutual    network    oil    oliv    oper    probabilist    rank    retriev    score    supervis    system    web
--------------------  -------  -----------  ---------  ---------  --------  ------  --------  ------  --------  -------  ------  --------  -------  --------  ---------  -----  ------  ------  -------------  ------  ---------  -------  ----------  --------  -----
Original method          7086        35062       8085       4671      1888   16595     11391    4513     17440     6003   12090      9122    15337      2708      17820  14270    3023   16591           1230    3643      13560     4179         588     38683  20128
New method               7113        35326       8164       4716      1891   16708     11411    4526     17512     6028   12306      9160    15422      2711      17968  14409    3027   16622           1229    3680      13570     4198         589     39351  20188
Frequency difference       27          264         79         45         3     113        20      13        72       25     216        38       85         3        148    139       4      31             -1      37         10       19           1       668     60
----------------------------------------
Overall difference: 2119

On remarque que la deuxieme methode permet de recuperer +2119 mots de la query !!

?? Nous avons remarqué un probleme tout les mots de la collection pas recupérer regex ajoute pas espace
-> Ajout espace dans la regex
clean_text = self.tag_pattern.sub('', ET.tostring(element, encoding='unicode'))

Maintenant:
-> clean_text = self.tag_pattern.sub(' ', ET.tostring(element, encoding='unicode'))

Apres supression du mots information des stopwords et la recupertion de tout les mots correctement:

                        actor    algorithm    analysi    benefit    exclus    film    health    hill    inform    learn    link    machin    model    mutual    network    oil    oliv    oper    probabilist    rank    retriev    score    supervis    system    web
--------------------  -------  -----------  ---------  ---------  --------  ------  --------  ------  --------  -------  ------  --------  -------  --------  ---------  -----  ------  ------  -------------  ------  ---------  -------  ----------  --------  -----
Original method          7115        35312       8143       4716      1893   16708     11414    4526     17490     6027   12306      9165    15424      2711      17912  14407    3025   16621           1229    3673      13578     4198         589     38947  20188
New method               7113        35326       8164       4716      1891   16708     11411    4526     17512     6028   12306      9160    15422      2711      17968  14409    3027   16622           1229    3680      13570     4198         589     39351  20188
Frequency difference       -2           14         21          0        -2       0        -3       0        22        1       0        -5       -2         0         56      2       2       1              0       7         -8        0           0       404      0
----------------------------------------
Overall difference: 508

Donc on a gagné 508 mots en plus dans la query en parsant correctement les documents cet fois.
Donc la nouvelle methode de preprocessing nous fait gagner 508 mots en plus. 
Mais en réalité c'est plus de 2119 qui sont ajouter du fait que la regex ne prenait pas en compte tout les mots de la collection.

Voici les stats de la collection sur ancienne methode avec preprocess reference suite a la correction de la regex:
╒═════════════════════════╤════════════╕
│ Statistic               │   Value    │
╞═════════════════════════╪════════════╡
│ Average document length │  650.929   │
├─────────────────────────┼────────────┤
│ Average word length     │  8.42026   │
├─────────────────────────┼────────────┤
│ Total unique words      │   211770   │
├─────────────────────────┼────────────┤
│ Total words             │ 6.3817e+06 │
╘═════════════════════════╧════════════╛
c'est donc environ 150000 mots de plus récupérer dans la collection.

======

En utilisant la loi de zipf on peut remarqué que le slope à été réduit grace a notre nouvelle methode :

Estimated slope for Reference collection (power law exponent): -1.368963310392746
Slope + 1: 0.3689633103927461
---------
Estimated slope for Further processed collection (power law exponent): -1.332048566362227
Slope + 1: 0.332048566362227

Une 3eme methode de preprocessing:
----------------------------------
- On a remarqué qu'il y avait pas mal de données "abérantes" dans la collection,
par exemple des mots dont la taille est supérieur a 30 caractères, ou des mots qui sont composé de 2 caractères.
On a donc exploré les données et pour les long mots la plupart du temps c'est des urls et pour les mots constitué de 2 caractères
ce sont principalement des mots tel que f_x p_x ect... qui provienne de formule mathématique.
Nous avons également décidé de les supprimer.

De plus nous avons observer qu'il y avais énormément de mots avec une fréquence inférieur a 5, 
nous avons donc décidé de les supprimer également car pour la plupart du temps ce sont des mots mal formé ou des mots qui ont été mal tokenizé.
par exemple: vrbgkt, sctcb, ovccb ect... 
Nous avons également supprimé ces mots.

Voici donc les nouvelles stats de la collection avec ces modifications:

╒═════════════════════════╤═════════════╕
│ Statistic               │    Value    │
╞═════════════════════════╪═════════════╡
│ Average document length │   580.832   │
├─────────────────────────┼─────────────┤
│ Average word length     │   6.68121   │
├─────────────────────────┼─────────────┤
│ Total unique words      │    40763    │
├─────────────────────────┼─────────────┤
│ Total words             │ 5.69448e+06 │
╘═════════════════════════╧═════════════╛

Donc on réduit encore la taille de la collection.
Et surtout on réduit le vocabulaire de la collection.